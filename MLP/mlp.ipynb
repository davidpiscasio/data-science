{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "In this notebook, we build a multilayer perceptron for digit recognition trained on the MNIST dataset. We used [Deep-Learning-Experiments](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/mlp/python/mlp_pytorch_demo.ipynb) as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from einops import rearrange\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple 4-layer multilayer perceptron for digit recognition. We use torch.nn.Module as superclass to remove boilerplate code. The number of input features corresponds to the input image size for the MNIST dataset, which is $28\\times28$ (It is grayscale so we do not need to multiply by 3). The number of nodes in the hidden layers is set to $256$. Finally, the number of classes is set to $10$, corresponding to the number of possible digits in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features=28*28, num_hidden=256, num_classes=10):\n",
    "        # Initiate the nn.Module superclass\n",
    "        super().__init__()\n",
    "\n",
    "        # Build the layers of the MLP\n",
    "        self.fc_in = nn.Linear(num_features,num_hidden)\n",
    "        self.fc_hid = nn.Linear(num_hidden,num_hidden)\n",
    "        self.fc_out = nn.Linear(num_hidden,num_classes)\n",
    "\n",
    "        # Set up the activation function (we choose ReLU) and the softmax function (for the output).\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1) # Comment if using CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input x from bx1x28x28 to 1x1*28*28=784 to match num_features.\n",
    "        y = rearrange(x, 'b c h w -> b (c h w)')\n",
    "\n",
    "        # Feed the rearranged input data to the input layer, then feed to the activation function.\n",
    "        y = self.fc_in(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        # Do the same for the 2 hidden layers.\n",
    "        y = self.fc_hid(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc_hid(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        # Feed the resulting tensor into the output layer. ReLU activation function is not needed since softmax is used for it.\n",
    "        y = self.fc_out(y)\n",
    "        y = self.softmax(y) # Comment if using CrossEntropyLoss()\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the necessary preparations for the dataset and training, using the PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTMLPModel(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001, batch_size=64, num_workers=1, max_epochs=30, model=MultilayerPerceptron, optim=\"adam\"):\n",
    "        # Initiate LightningModule superclass\n",
    "        super().__init__()\n",
    "        self.train_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        # Set up other parameters\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model()\n",
    "\n",
    "        # Set up loss function (Mean Squared Error) and accuracy\n",
    "        self.loss = nn.MSELoss()\n",
    "        #self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.optim = optim \n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Perform one-hot encoding on y first\n",
    "        y = self.mnist_one_hot(y) # Comment if using CrossEntropyLoss()\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.train_step_outputs.append({\"loss\": loss})\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in self.train_step_outputs]).mean()\n",
    "        print(f'Train loss: {avg_loss}')\n",
    "        self.train_step_outputs.clear()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Perform one-hot encoding on y first. We keep the original y for the accuracy\n",
    "        y_oh = self.mnist_one_hot(y) # Comment if using CrossEntropyLoss()\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y_oh) # Comment if using CrossEntropyLoss()\n",
    "        #loss = self.loss(y_hat, y) # Uncomment if using CrossEntropyLoss()\n",
    "        # We get the predictions through argmax\n",
    "        y_preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.accuracy(y, y_preds) * 100. # Comment if using CrossEntropyLoss()\n",
    "        #acc = self.accuracy(y, y_hat) * 100. # Uncomment if using CrossEntropyLoss()\n",
    "        self.test_step_outputs.append({\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc})\n",
    "        return y_hat, loss, acc\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in self.test_step_outputs]).mean()\n",
    "        print(f'Test loss: {avg_loss}')\n",
    "        print(f'Test accuracy: {avg_acc}')\n",
    "        self.test_step_outputs.clear()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        return self.on_test_epoch_end()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optim == \"adam\":\n",
    "            optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        elif self.optim == \"sgd\":\n",
    "            optimizer = SGD(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # Settings from https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                                        transform=torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.ToTensor(),\n",
    "                                        torchvision.transforms.Normalize((0.1307,), (0.3081,))])), \n",
    "                                        batch_size=self.hparams.batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                                        transform=torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.ToTensor(),\n",
    "                                        torchvision.transforms.Normalize((0.1307,), (0.3081,))])), \n",
    "                                        batch_size=self.hparams.batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataloader()\n",
    "        self.test_dataloader()\n",
    "\n",
    "    # Perform MNIST-specific one-hot encoding\n",
    "    def mnist_one_hot(self, x):\n",
    "        device = 'cuda' if 'cuda' in str(x.device) else 'cpu'\n",
    "        y_oh = torch.zeros(size=(x.shape[0],10), device=device)\n",
    "        y_oh.to(device)\n",
    "        for i, y in enumerate(x):\n",
    "            y_oh[i,y] = 1\n",
    "        return y_oh.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the model, the datasets, and the train/test/validation configurations, we set up the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"PyTorch Lightning MNIST Example\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"num epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=10, help=\"num classes\")\n",
    "\n",
    "    parser.add_argument(\"--optim\", default=\"adam\", help=\"optimizer\")\n",
    "    # Verify device count with torch.cuda.device_count()\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    # Verify CUDA availability with torch.cuda.is_available())\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    # Recommended: num_workers = (os.cpu_count() // 2) // torch.cuda.device_count()\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"num workers\")\n",
    "\n",
    "    parser.add_argument(\"--model\", default=MultilayerPerceptron)\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the MLP model with the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTMLPModel(\n",
      "  (model): MultilayerPerceptron(\n",
      "    (fc_in): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (fc_hid): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (loss): MSELoss()\n",
      "  (accuracy): MulticlassAccuracy()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | model    | MultilayerPerceptron | 269 K \n",
      "1 | loss     | MSELoss              | 0     \n",
      "2 | accuracy | MulticlassAccuracy   | 0     \n",
      "--------------------------------------------------\n",
      "269 K     Trainable params\n",
      "0         Non-trainable params\n",
      "269 K     Total params\n",
      "1.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]Test loss: 0.09025193750858307\n",
      "Test accuracy: 7.8125\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:21<00:00, 44.30it/s, v_num=41]Test loss: 0.006266745738685131\n",
      "Test accuracy: 95.85987091064453\n",
      "Epoch 0: 100%|██████████| 938/938 [00:25<00:00, 37.04it/s, v_num=41, test_loss=0.00627, test_acc=95.90]Train loss: 0.011967782862484455\n",
      "Epoch 1: 100%|██████████| 938/938 [00:24<00:00, 38.74it/s, v_num=41, test_loss=0.00627, test_acc=95.90]Test loss: 0.005569866858422756\n",
      "Test accuracy: 96.26791381835938\n",
      "Epoch 1: 100%|██████████| 938/938 [00:28<00:00, 33.00it/s, v_num=41, test_loss=0.00557, test_acc=96.30]Train loss: 0.005723630078136921\n",
      "Epoch 2: 100%|██████████| 938/938 [00:23<00:00, 39.77it/s, v_num=41, test_loss=0.00557, test_acc=96.30]Test loss: 0.004178702365607023\n",
      "Test accuracy: 97.31289672851562\n",
      "Epoch 2: 100%|██████████| 938/938 [00:26<00:00, 35.15it/s, v_num=41, test_loss=0.00418, test_acc=97.30]Train loss: 0.004358722362667322\n",
      "Epoch 3: 100%|██████████| 938/938 [00:22<00:00, 41.85it/s, v_num=41, test_loss=0.00418, test_acc=97.30]Test loss: 0.00478748232126236\n",
      "Test accuracy: 96.91481018066406\n",
      "Epoch 3: 100%|██████████| 938/938 [00:26<00:00, 35.23it/s, v_num=41, test_loss=0.00479, test_acc=96.90]Train loss: 0.0037324929144233465\n",
      "Epoch 4: 100%|██████████| 938/938 [00:23<00:00, 40.19it/s, v_num=41, test_loss=0.00479, test_acc=96.90]Test loss: 0.003573344787582755\n",
      "Test accuracy: 97.73089599609375\n",
      "Epoch 4: 100%|██████████| 938/938 [00:27<00:00, 34.11it/s, v_num=41, test_loss=0.00357, test_acc=97.70]Train loss: 0.0031836063135415316\n",
      "Epoch 5: 100%|██████████| 938/938 [00:24<00:00, 38.14it/s, v_num=41, test_loss=0.00357, test_acc=97.70]Test loss: 0.004506420344114304\n",
      "Test accuracy: 97.11385345458984\n",
      "Epoch 5: 100%|██████████| 938/938 [00:28<00:00, 32.57it/s, v_num=41, test_loss=0.00451, test_acc=97.10]Train loss: 0.0028399741277098656\n",
      "Epoch 6: 100%|██████████| 938/938 [00:24<00:00, 38.70it/s, v_num=41, test_loss=0.00451, test_acc=97.10]Test loss: 0.005269884597510099\n",
      "Test accuracy: 96.68590545654297\n",
      "Epoch 6: 100%|██████████| 938/938 [00:28<00:00, 33.00it/s, v_num=41, test_loss=0.00527, test_acc=96.70]Train loss: 0.0025886869989335537\n",
      "Epoch 7: 100%|██████████| 938/938 [00:23<00:00, 39.22it/s, v_num=41, test_loss=0.00527, test_acc=96.70]Test loss: 0.0038583248388022184\n",
      "Test accuracy: 97.66122436523438\n",
      "Epoch 7: 100%|██████████| 938/938 [00:28<00:00, 33.46it/s, v_num=41, test_loss=0.00386, test_acc=97.70]Train loss: 0.002292945282533765\n",
      "Epoch 8: 100%|██████████| 938/938 [00:23<00:00, 40.18it/s, v_num=41, test_loss=0.00386, test_acc=97.70]Test loss: 0.003772822907194495\n",
      "Test accuracy: 97.6811294555664\n",
      "Epoch 8: 100%|██████████| 938/938 [00:27<00:00, 34.08it/s, v_num=41, test_loss=0.00377, test_acc=97.70]Train loss: 0.002070924500003457\n",
      "Epoch 9: 100%|██████████| 938/938 [00:23<00:00, 40.30it/s, v_num=41, test_loss=0.00377, test_acc=97.70]Test loss: 0.0039093405939638615\n",
      "Test accuracy: 97.65127563476562\n",
      "Epoch 9: 100%|██████████| 938/938 [00:27<00:00, 34.16it/s, v_num=41, test_loss=0.00391, test_acc=97.70]Train loss: 0.0018717754865065217\n",
      "Epoch 10: 100%|██████████| 938/938 [00:23<00:00, 40.74it/s, v_num=41, test_loss=0.00391, test_acc=97.70]Test loss: 0.0031425149645656347\n",
      "Test accuracy: 98.08917236328125\n",
      "Epoch 10: 100%|██████████| 938/938 [00:27<00:00, 34.44it/s, v_num=41, test_loss=0.00314, test_acc=98.10]Train loss: 0.0016817925497889519\n",
      "Epoch 11: 100%|██████████| 938/938 [00:24<00:00, 38.70it/s, v_num=41, test_loss=0.00314, test_acc=98.10]Test loss: 0.003303041448816657\n",
      "Test accuracy: 98.03941345214844\n",
      "Epoch 11: 100%|██████████| 938/938 [00:28<00:00, 32.98it/s, v_num=41, test_loss=0.0033, test_acc=98.00] Train loss: 0.001431888435035944\n",
      "Epoch 12: 100%|██████████| 938/938 [00:22<00:00, 41.59it/s, v_num=41, test_loss=0.0033, test_acc=98.00]Test loss: 0.0033112065866589546\n",
      "Test accuracy: 98.03941345214844\n",
      "Epoch 12: 100%|██████████| 938/938 [00:26<00:00, 35.11it/s, v_num=41, test_loss=0.00331, test_acc=98.00]Train loss: 0.0012303921394050121\n",
      "Epoch 13: 100%|██████████| 938/938 [00:24<00:00, 38.23it/s, v_num=41, test_loss=0.00331, test_acc=98.00]Test loss: 0.0030048824846744537\n",
      "Test accuracy: 98.1886978149414\n",
      "Epoch 13: 100%|██████████| 938/938 [00:28<00:00, 33.50it/s, v_num=41, test_loss=0.003, test_acc=98.20]  Train loss: 0.00116779375821352\n",
      "Epoch 14: 100%|██████████| 938/938 [00:23<00:00, 40.09it/s, v_num=41, test_loss=0.003, test_acc=98.20]Test loss: 0.003298925468698144\n",
      "Test accuracy: 97.97969818115234\n",
      "Epoch 14: 100%|██████████| 938/938 [00:27<00:00, 34.11it/s, v_num=41, test_loss=0.0033, test_acc=98.00]Train loss: 0.0009681253577582538\n",
      "Epoch 15: 100%|██████████| 938/938 [00:22<00:00, 42.11it/s, v_num=41, test_loss=0.0033, test_acc=98.00]Test loss: 0.002759270602837205\n",
      "Test accuracy: 98.3777847290039\n",
      "Epoch 15: 100%|██████████| 938/938 [00:26<00:00, 35.34it/s, v_num=41, test_loss=0.00276, test_acc=98.40]Train loss: 0.0009376744856126606\n",
      "Epoch 16: 100%|██████████| 938/938 [00:24<00:00, 38.56it/s, v_num=41, test_loss=0.00276, test_acc=98.40]Test loss: 0.0034262770786881447\n",
      "Test accuracy: 97.99960327148438\n",
      "Epoch 16: 100%|██████████| 938/938 [00:28<00:00, 32.82it/s, v_num=41, test_loss=0.00343, test_acc=98.00]Train loss: 0.0008011229801923037\n",
      "Epoch 17: 100%|██████████| 938/938 [00:23<00:00, 40.06it/s, v_num=41, test_loss=0.00343, test_acc=98.00]Test loss: 0.003060651943087578\n",
      "Test accuracy: 98.16879272460938\n",
      "Epoch 17: 100%|██████████| 938/938 [00:27<00:00, 34.22it/s, v_num=41, test_loss=0.00306, test_acc=98.20]Train loss: 0.0006680806400254369\n",
      "Epoch 18: 100%|██████████| 938/938 [00:23<00:00, 39.69it/s, v_num=41, test_loss=0.00306, test_acc=98.20]Test loss: 0.0027458767872303724\n",
      "Test accuracy: 98.33798217773438\n",
      "Epoch 18: 100%|██████████| 938/938 [00:27<00:00, 34.42it/s, v_num=41, test_loss=0.00275, test_acc=98.30]Train loss: 0.0005644822958856821\n",
      "Epoch 19: 100%|██████████| 938/938 [00:23<00:00, 39.77it/s, v_num=41, test_loss=0.00275, test_acc=98.30]Test loss: 0.0030385181307792664\n",
      "Test accuracy: 98.268310546875\n",
      "Epoch 19: 100%|██████████| 938/938 [00:27<00:00, 34.09it/s, v_num=41, test_loss=0.00304, test_acc=98.30]Train loss: 0.0004925562534481287\n",
      "Epoch 20: 100%|██████████| 938/938 [00:21<00:00, 43.00it/s, v_num=41, test_loss=0.00304, test_acc=98.30]Test loss: 0.002870015799999237\n",
      "Test accuracy: 98.29817199707031\n",
      "Epoch 20: 100%|██████████| 938/938 [00:25<00:00, 36.12it/s, v_num=41, test_loss=0.00287, test_acc=98.30]Train loss: 0.00044963916298002005\n",
      "Epoch 21: 100%|██████████| 938/938 [00:23<00:00, 39.48it/s, v_num=41, test_loss=0.00287, test_acc=98.30]Test loss: 0.0027316929772496223\n",
      "Test accuracy: 98.3777847290039\n",
      "Epoch 21: 100%|██████████| 938/938 [00:27<00:00, 33.56it/s, v_num=41, test_loss=0.00273, test_acc=98.40]Train loss: 0.00038188742473721504\n",
      "Epoch 22: 100%|██████████| 938/938 [00:23<00:00, 40.02it/s, v_num=41, test_loss=0.00273, test_acc=98.40]Test loss: 0.002855803119018674\n",
      "Test accuracy: 98.27826690673828\n",
      "Epoch 22: 100%|██████████| 938/938 [00:27<00:00, 33.89it/s, v_num=41, test_loss=0.00286, test_acc=98.30]Train loss: 0.00035279063740745187\n",
      "Epoch 23: 100%|██████████| 938/938 [00:22<00:00, 41.03it/s, v_num=41, test_loss=0.00286, test_acc=98.30]Test loss: 0.0026646468322724104\n",
      "Test accuracy: 98.42755126953125\n",
      "Epoch 23: 100%|██████████| 938/938 [00:26<00:00, 34.76it/s, v_num=41, test_loss=0.00266, test_acc=98.40]Train loss: 0.0003273954789619893\n",
      "Epoch 24: 100%|██████████| 938/938 [00:23<00:00, 40.69it/s, v_num=41, test_loss=0.00266, test_acc=98.40]Test loss: 0.0026202714070677757\n",
      "Test accuracy: 98.46736145019531\n",
      "Epoch 24: 100%|██████████| 938/938 [00:27<00:00, 34.41it/s, v_num=41, test_loss=0.00262, test_acc=98.50]Train loss: 0.00031332357320934534\n",
      "Epoch 25: 100%|██████████| 938/938 [00:23<00:00, 39.71it/s, v_num=41, test_loss=0.00262, test_acc=98.50]Test loss: 0.002727111801505089\n",
      "Test accuracy: 98.39768981933594\n",
      "Epoch 25: 100%|██████████| 938/938 [00:27<00:00, 33.92it/s, v_num=41, test_loss=0.00273, test_acc=98.40]Train loss: 0.0003080391325056553\n",
      "Epoch 26: 100%|██████████| 938/938 [00:23<00:00, 39.29it/s, v_num=41, test_loss=0.00273, test_acc=98.40]Test loss: 0.002633890835568309\n",
      "Test accuracy: 98.45740509033203\n",
      "Epoch 26: 100%|██████████| 938/938 [00:28<00:00, 33.46it/s, v_num=41, test_loss=0.00263, test_acc=98.50]Train loss: 0.000299334671581164\n",
      "Epoch 27: 100%|██████████| 938/938 [00:23<00:00, 39.21it/s, v_num=41, test_loss=0.00263, test_acc=98.50]Test loss: 0.002590035554021597\n",
      "Test accuracy: 98.52706909179688\n",
      "Epoch 27: 100%|██████████| 938/938 [00:28<00:00, 33.21it/s, v_num=41, test_loss=0.00259, test_acc=98.50]Train loss: 0.0002926086017396301\n",
      "Epoch 28: 100%|██████████| 938/938 [00:23<00:00, 39.62it/s, v_num=41, test_loss=0.00259, test_acc=98.50]Test loss: 0.0025964623782783747\n",
      "Test accuracy: 98.51712036132812\n",
      "Epoch 28: 100%|██████████| 938/938 [00:27<00:00, 33.62it/s, v_num=41, test_loss=0.0026, test_acc=98.50] Train loss: 0.00028957423637621105\n",
      "Epoch 29: 100%|██████████| 938/938 [00:23<00:00, 39.22it/s, v_num=41, test_loss=0.0026, test_acc=98.50]Test loss: 0.002599412575364113\n",
      "Test accuracy: 98.4972152709961\n",
      "Epoch 29: 100%|██████████| 938/938 [00:28<00:00, 33.49it/s, v_num=41, test_loss=0.0026, test_acc=98.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.000287483970168978\n",
      "Epoch 29: 100%|██████████| 938/938 [00:28<00:00, 33.46it/s, v_num=41, test_loss=0.0026, test_acc=98.50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:03<00:00, 42.92it/s]Test loss: 0.002599412575364113\n",
      "Test accuracy: 98.4972152709961\n",
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:03<00:00, 42.80it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             98.4972152709961\n",
      "        test_loss          0.002599412575364113\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    model = MNISTMLPModel(lr=args.lr, batch_size=args.batch_size,\n",
    "                           num_workers=args.num_workers,\n",
    "                           model=args.model, optim=args.optim)\n",
    "    model.setup()\n",
    "    print(model)\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      max_epochs=args.epochs)\n",
    "\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes (Output text files are available in the same folder as this notebook)\n",
    "* Adam optimizer performs significantly better than Stochastic Gradient Descent (SGD) in terms of accuracy. This may be due to the claim of various studies that Adam converges faster than SGD (source: [Adam vs SGD](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)).\n",
    "* Both Mean-Squared Error (MSE) and Cross Entropy (CE) loss functions perform well when used with the Adam optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
